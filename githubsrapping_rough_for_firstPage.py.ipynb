{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42461f7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d781a94c",
   "metadata": {},
   "source": [
    "## Pick a website and describe your objective\n",
    "* Browse through different sites and pick on to scrape. Check the \"Project Ideas\" section for inspiration.\n",
    "\n",
    "* Identify the information you'd like to scrape from the site. Decide the format of the output CSV file.\n",
    "\n",
    "* Summarize your project idea and outline your strategy in a Juptyer notebook. Use the \"New\" button above.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669a885f",
   "metadata": {},
   "source": [
    "## project outline:\n",
    "* we are going to scrape url \"https://github.com/topics\"\n",
    "* we will get the topic and get topic title and url link\n",
    "* we will get the of each topic of top 25 repositeries\n",
    "* we extract the data reponame , user name , stars, url link \n",
    "* create the csv file for each topics \n",
    "\" RepoName,UserName,Stars,Repo_url\n",
    "metafizzy,infinite-scroll,6900,https://github.com/metafizzy/infinite-scroll\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0b47370",
   "metadata": {},
   "source": [
    "## Use the requests library to download web pages\n",
    "* Inspect the website's HTML source and identify the right URLs to download.\n",
    "* Download and save web pages locally using the requests library.\n",
    "* Create a function to automate downloading for different topics/search queries.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0c98f6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "99db1e91",
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_url=\"https://github.com/topics\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f54cf50d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response=requests.get(topics_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "82008fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "922f31c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "255fc072",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(response.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7594a8f1",
   "metadata": {},
   "source": [
    "# html content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "243060da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_content=response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c21536d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# page_content[:1000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8927968",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# import io\n",
    "# file = io.open('webpage.html', \"w\", encoding=\"utf-8\")\n",
    "# file.write(page_content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8aca737",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1a0364e9",
   "metadata": {},
   "source": [
    "## Use Beautiful Soup to parse and extract information\n",
    "* Parse and explore the structure of downloaded web pages using Beautiful soup.\n",
    "* Use the right properties and methods to extract the required information.\n",
    "* Create functions to extract from the page into lists and dictionaries.\n",
    "* (Optional) Use a REST API to acquire additional information if required.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "35da8b0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4511477",
   "metadata": {},
   "source": [
    "# Parsed the html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7d80632a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup=BeautifulSoup(page_content, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "676bdb1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# soup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6397160f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptag=soup.find_all('p')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a51e4fbd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(ptag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b076904d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ptag[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "36c2b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selection_class=\"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "# topic_title_ptag=soup.find_all('p',{'class': selection_class})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "695dcfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_title_ptag=soup.find_all('p', class_=selection_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3e8614ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(topic_title_ptag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "fd7f95a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_title_ptag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e0cffeb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# desc_selector=\"f5 color-text-secondary mb-0 mt-1\"\n",
    "# topic_dis_tag=soup.find_all('p', class_=desc_selector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "46e99872",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(topic_dis_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7b97abf8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_dis_tag[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6b494b04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_dis_tag0=topic_dis_tag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ea43bf48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to parents of first topic_dis_tag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e5f4f670",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_dis_tag0.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "6e31757d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_dis_tag0.parent.parent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8142156b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "256928f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_link_atag=soup.find_all('a',{'class': \"d-flex no-underline\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "08a45cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(topic_lik_atag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6ad0b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_lik_atag[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "9ce3fe23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_lik_atag[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "07f0f7c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic0_url=\"https://github.com/\"+topic_lik_atag[0]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "56b78f5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic0_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8a3c7ed3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_title_ptag[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a353cf28",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2bc3ef2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_title=[]\n",
    "# for title in topic_title_ptag:\n",
    "#     topic_title.append(title.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "30041b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "663a600e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_discriptions=[]\n",
    "# for dic in topic_dis_tag:\n",
    "#     topic_discriptions.append(dic.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d4d242",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "c144ee98",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_discriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0b5d9c93",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_discriptions=[]\n",
    "# for dic in topic_dis_tag:\n",
    "#     topic_discriptions.append(dic.text.strip())\n",
    "# topic_discriptions[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ec6123",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "92b146f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_url=[]\n",
    "# base_url='https://github.com/'\n",
    "# for link in topic_link_atag:\n",
    "#     topic_url.append(base_url + link['href'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7b0fe1d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_url"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bf7df9",
   "metadata": {},
   "source": [
    "# create the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "bd384c49",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_dict={'title': topic_title, 'dicription': topic_discriptions, 'url_link': topic_url}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "fd958863",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "82c9d94d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_df= pd.DataFrame(topic_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "51e526fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4bdf700",
   "metadata": {},
   "source": [
    "## Create CSV file(s) with the extracted information\n",
    "* Create functions for the end-to-end process of downloading, parsing, and saving CSVs.\n",
    "* Execute the function with different inputs to create a dataset of CSV files.\n",
    "* Verify the information in the CSV files by reading them back using Pandas.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "0d381299",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_df.to_csv('git_topi.csv', index = None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25368d36",
   "metadata": {},
   "source": [
    "## Getting information out of topic page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "87e81767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_page_url= topic_url[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "374afacc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_page_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "0f9ec478",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response= requests.get(topic_page_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "abb2b102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# response.status_code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "a0c93105",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "f19737ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_doc=BeautifulSoup(response.text,'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7b86227c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_tags=topic_doc.find_all('h1', class_=\"f3 color-text-secondary text-normal lh-condensed\" )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "44843698",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(repo_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "3801a76d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_tags[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "70f45496",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_tags=repo_tags[0].find_all('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c78ecc8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "070f36c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_tags[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "29803a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_tags[1].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e82208a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a_tags[1]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "86536005",
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "d99e904a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo_url=base_url+a_tags[1]['href']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "8563addf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_selection=\"social-count float-none\"\n",
    "# star_tags=topic_doc.find_all('a', {'class' : star_selection})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bd7cadbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# len(star_tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "b3a45e7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "951f0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# star_tags[0].text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "a4747b88",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_star_count(star_str):\n",
    "    star_str=star_str.strip()\n",
    "    if star_str[-1]=='k':\n",
    "        return int(float(star_str[:-1])*1000)\n",
    "    return int(star_str)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "b55fce2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# parse_star_count(star_tags[0].text.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "4342b25f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_repo_info(h1_tags, star_tags):\n",
    "#     # return all the info about the repository \n",
    "#     a_tags=h1_tags.find_all('a')\n",
    "#     user_name=a_tags[0].text.strip()\n",
    "#     repo_name=a_tags[1].text.strip()\n",
    "#     repo_url=base_url+a_tags[1]['href']\n",
    "#     stars= parse_star_count(star_tags.text.strip())\n",
    "#     return user_name, repo_name, stars, repo_url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3767a36a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_repo_info(repo_tags[0], star_tags[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "ab920328",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_repo_dict={'user_name':[], 'repo_name':[], 'star':[], 'repo_url':[]\n",
    "                \n",
    "#                 }\n",
    "# for i in range(len(repo_tags)):\n",
    "#     repo_info=get_repo_info(repo_tags[i], star_tags[i])\n",
    "#     topic_repo_dict['user_name'].append(repo_info[0])\n",
    "#     topic_repo_dict['repo_name'].append(repo_info[1])\n",
    "#     topic_repo_dict['star'].append(repo_info[2])\n",
    "#     topic_repo_dict['repo_url'].append(repo_info[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "55f6307d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_repo_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "9c36147e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_repo_df=pd.DataFrame(topic_repo_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "5f4bc277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_repo_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "f63d354a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def get_topic_page(topic_url):\n",
    "#     #download the page\n",
    "#     response=requests.get(topic_url)\n",
    "#     #check response\n",
    "#     if response.status_code != 200 :\n",
    "#         raise Exception(f'Failed to load page {topic_url}')\n",
    "#         #parser with beautifulsoup\n",
    "#     topic_doc=BeautifulSoup(response.text, 'html.parser')\n",
    "#         #get h1 tags containing repo title , repo url, user name\n",
    "#     return topic_doc\n",
    "\n",
    "# def get_repo_info(h1_tags, star_tags):\n",
    "#     # return all the info about the repository \n",
    "#     a_tags=h1_tags.find_all('a')\n",
    "#     user_name=a_tags[0].text.strip()\n",
    "#     repo_name=a_tags[1].text.strip()\n",
    "#     base_url='https://github.com/'\n",
    "#     repo_url=base_url+a_tags[1]['href']\n",
    "#     stars= parse_star_count(star_tags.text.strip())\n",
    "#     return user_name, repo_name, stars, repo_url\n",
    "\n",
    "# def get_topic_repo(topic_doc):\n",
    "#     #containging repo tile, user name and repo url \n",
    "#     repo_tags=topic_doc.find_all('h1', class_=\"f3 color-text-secondary text-normal lh-condensed\" )\n",
    "#     #get star tags\n",
    "#     star_selection=\"social-count float-none\"\n",
    "#     star_tags=topic_doc.find_all('a', {'class' : star_selection})\n",
    "#     #get all the repository information\n",
    "        \n",
    "#     topic_repo_dict={'user_name':[], 'repo_name':[], 'star':[], 'repo_url':[]\n",
    "                \n",
    "#                 }\n",
    "#     for i in range(len(repo_tags)):\n",
    "#         repo_info=get_repo_info(repo_tags[i], star_tags[i])\n",
    "#         topic_repo_dict['user_name'].append(repo_info[0])\n",
    "#         topic_repo_dict['repo_name'].append(repo_info[1])\n",
    "#         topic_repo_dict['star'].append(repo_info[2])\n",
    "#         topic_repo_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "#     return pd.DataFrame(topic_repo_dict)\n",
    "\n",
    "# def scrape_topic(topic_url, topic_name):\n",
    "#     topic_df=get_topic_repo(get_topic_page(topic_url))\n",
    "#     topic_df.to_csv(topic_name+'.csv', index=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d124d675",
   "metadata": {},
   "source": [
    "## Finalcode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cc74fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "97110572",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "def get_topic_page(topic_url):\n",
    "    #download the page\n",
    "    response=requests.get(topic_url)\n",
    "    #check response\n",
    "    if response.status_code != 200 :\n",
    "        raise Exception(f'Failed to load page {topic_url}')\n",
    "        #parser with beautifulsoup\n",
    "    topic_doc=BeautifulSoup(response.text, 'html.parser')\n",
    "        #get h1 tags containing repo title , repo url, user name\n",
    "    return topic_doc\n",
    "\n",
    "def get_repo_info(h1_tags, star_tags):\n",
    "    # return all the info about the repository \n",
    "    a_tags=h1_tags.find_all('a')\n",
    "    user_name=a_tags[0].text.strip()\n",
    "    repo_name=a_tags[1].text.strip()\n",
    "    base_url='https://github.com/'\n",
    "    repo_url=base_url+a_tags[1]['href']\n",
    "    stars= parse_star_count(star_tags.text.strip())\n",
    "    return user_name, repo_name, stars, repo_url\n",
    "\n",
    "def get_topic_repo(topic_doc):\n",
    "    #containging repo tile, user name and repo url \n",
    "    repo_tags=topic_doc.find_all('h1', class_=\"f3 color-text-secondary text-normal lh-condensed\" )\n",
    "    #get star tags\n",
    "    star_selection=\"social-count float-none\"\n",
    "    star_tags=topic_doc.find_all('a', {'class' : star_selection})\n",
    "    #get all the repository information\n",
    "        \n",
    "    topic_repo_dict={'user_name':[], 'repo_name':[], 'star':[], 'repo_url':[]\n",
    "                \n",
    "                }\n",
    "    for i in range(len(repo_tags)):\n",
    "        repo_info=get_repo_info(repo_tags[i], star_tags[i])\n",
    "        topic_repo_dict['user_name'].append(repo_info[0])\n",
    "        topic_repo_dict['repo_name'].append(repo_info[1])\n",
    "        topic_repo_dict['star'].append(repo_info[2])\n",
    "        topic_repo_dict['repo_url'].append(repo_info[3])\n",
    "        \n",
    "    return pd.DataFrame(topic_repo_dict)\n",
    "\n",
    "def scrape_topic(topic_url, path):\n",
    "    \n",
    "    #fname=topic_name+'.csv'#change from fname to path\n",
    "    if os.path.exists(path):\n",
    "        print('The file {} already exists skipping....'.format(path))\n",
    "    else:\n",
    "        topic_df=get_topic_repo(get_topic_page(topic_url))\n",
    "        topic_df.to_csv(path, index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "83c1a31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url4 = topic_url[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "03777c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# url4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9c46e729",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic4_doc= get_topic_page(url4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "d29db100",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic4_repos=get_topic_repo(topic4_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "0e93178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic4_repos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8c23ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_url[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "49464fa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_topic_repo(get_topic_page(topic_url[6])).to_csv('ansible.csv', index= None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46d07bc5",
   "metadata": {},
   "source": [
    "\n",
    "Write a single function to:\n",
    "  1. get the list of topic from the topics page \n",
    "  2. get the list of top repos from the indivisual topic page\n",
    "  3. for each topic create the csv of the top repo for the topic\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "2a3c7596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_topic_titles(doc):\n",
    "    selection_class=\"f3 lh-condensed mb-0 mt-1 Link--primary\"\n",
    "    topic_title_ptag=doc.find_all('p',{'class': selection_class})\n",
    "    topic_title=[]\n",
    "    for title in topic_title_ptag:\n",
    "        topic_title.append(title.text)\n",
    "    return topic_title\n",
    "\n",
    "def get_topic_disc(doc):\n",
    "    desc_selector=\"f5 color-text-secondary mb-0 mt-1\"\n",
    "    topic_dis_tag=doc.find_all('p', class_=desc_selector)\n",
    "    topic_discriptions=[]\n",
    "    for dic in topic_dis_tag:\n",
    "        topic_discriptions.append(dic.text.strip())\n",
    "    return topic_discriptions\n",
    "\n",
    "def get_topic_urls(doc):\n",
    "    topic_link_atag=doc.find_all('a',{'class': \"d-flex no-underline\"})\n",
    "    topic_url=[]\n",
    "    \n",
    "    base_url=\"https://github.com\"\n",
    "    for link in topic_link_atag:\n",
    "        topic_url.append(base_url + link['href'])\n",
    "    return topic_url\n",
    "    \n",
    "    \n",
    "def scrape_topics():\n",
    "    topics_url= 'https://github.com/topics'\n",
    "    response = requests.get(topics_url)\n",
    "    if response.status_code != 200 :\n",
    "        raise Exception(f'Failed to load page {topics_url}')\n",
    "        \n",
    "    doc=BeautifulSoup(response.text,'html.parser')\n",
    "    topics_dict={\n",
    "        'title': get_topic_titles(doc),\n",
    "        'description' : get_topic_disc(doc),\n",
    "        'url' : get_topic_urls(doc)\n",
    "    }\n",
    "    return pd.DataFrame(topics_dict)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "845c160c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrape_topics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "b3170ea3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function makedirs in module os:\n",
      "\n",
      "makedirs(name, mode=511, exist_ok=False)\n",
      "    makedirs(name [, mode=0o777][, exist_ok=False])\n",
      "    \n",
      "    Super-mkdir; create a leaf directory and all intermediate ones.  Works like\n",
      "    mkdir, except that any intermediate path segment (not just the rightmost)\n",
      "    will be created if it does not exist. If the target directory already\n",
      "    exists, raise an OSError if exist_ok is False. Otherwise no exception is\n",
      "    raised.  This is recursive.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "help(os.makedirs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e945e4ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_topic_repos():\n",
    "    print('scraping list of topics')\n",
    "    topic_df=scrape_topics()\n",
    "    os.makedirs('data', exist_ok=True)#creating folder for the store the data\n",
    "    for index, row in topic_df.iterrows():\n",
    "        print('scraping top repositories for \"{}\"'.format(row['title']))\n",
    "        scrape_topic(row['url'], 'data/{}.csv'.format(row['title']) )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "6de4f071",
   "metadata": {},
   "outputs": [],
   "source": [
    "# topic_df=scrape_topics()\n",
    "# for index, row in topic_df.iterrows():\n",
    "#     print(row['url'],row['title'])\n",
    "#     scrape_topic(row['url'], row['title'] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "02ddc96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # rough work\n",
    "# for index, row in topic_df.iterrows():\n",
    "#     print(row['url'],row['title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "14a9ddab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scraping list of topics\n",
      "scraping top repositories for \"3D\"\n",
      "The file data/3D.csv already exists skipping....\n",
      "scraping top repositories for \"Ajax\"\n",
      "The file data/Ajax.csv already exists skipping....\n",
      "scraping top repositories for \"Algorithm\"\n",
      "The file data/Algorithm.csv already exists skipping....\n",
      "scraping top repositories for \"Amp\"\n",
      "The file data/Amp.csv already exists skipping....\n",
      "scraping top repositories for \"Android\"\n",
      "The file data/Android.csv already exists skipping....\n",
      "scraping top repositories for \"Angular\"\n",
      "The file data/Angular.csv already exists skipping....\n",
      "scraping top repositories for \"Ansible\"\n",
      "The file data/Ansible.csv already exists skipping....\n",
      "scraping top repositories for \"API\"\n",
      "The file data/API.csv already exists skipping....\n",
      "scraping top repositories for \"Arduino\"\n",
      "The file data/Arduino.csv already exists skipping....\n",
      "scraping top repositories for \"ASP.NET\"\n",
      "The file data/ASP.NET.csv already exists skipping....\n",
      "scraping top repositories for \"Atom\"\n",
      "The file data/Atom.csv already exists skipping....\n",
      "scraping top repositories for \"Awesome Lists\"\n",
      "The file data/Awesome Lists.csv already exists skipping....\n",
      "scraping top repositories for \"Amazon Web Services\"\n",
      "The file data/Amazon Web Services.csv already exists skipping....\n",
      "scraping top repositories for \"Azure\"\n",
      "The file data/Azure.csv already exists skipping....\n",
      "scraping top repositories for \"Babel\"\n",
      "The file data/Babel.csv already exists skipping....\n",
      "scraping top repositories for \"Bash\"\n",
      "The file data/Bash.csv already exists skipping....\n",
      "scraping top repositories for \"Bitcoin\"\n",
      "The file data/Bitcoin.csv already exists skipping....\n",
      "scraping top repositories for \"Bootstrap\"\n",
      "The file data/Bootstrap.csv already exists skipping....\n",
      "scraping top repositories for \"Bot\"\n",
      "The file data/Bot.csv already exists skipping....\n",
      "scraping top repositories for \"C\"\n",
      "The file data/C.csv already exists skipping....\n",
      "scraping top repositories for \"Chrome\"\n",
      "The file data/Chrome.csv already exists skipping....\n",
      "scraping top repositories for \"Chrome extension\"\n",
      "The file data/Chrome extension.csv already exists skipping....\n",
      "scraping top repositories for \"Command line interface\"\n",
      "The file data/Command line interface.csv already exists skipping....\n",
      "scraping top repositories for \"Clojure\"\n",
      "The file data/Clojure.csv already exists skipping....\n",
      "scraping top repositories for \"Code quality\"\n",
      "The file data/Code quality.csv already exists skipping....\n",
      "scraping top repositories for \"Code review\"\n",
      "scraping top repositories for \"Compiler\"\n",
      "scraping top repositories for \"Continuous integration\"\n",
      "scraping top repositories for \"COVID-19\"\n",
      "scraping top repositories for \"C++\"\n"
     ]
    }
   ],
   "source": [
    "scrape_topic_repos()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e276351",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861ab4cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a483340",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
